# خلاصه تحلیلی مقاله

**عنوان:** Improving Generative Ad Text on Facebook using Reinforcement Learning (arXiv:2507.21983v2)

## ۱) مسئله اصلی و چرایی اهمیت
مدل‌های زبانی بزرگ در پیش‌آموزش، دانش عمومی زبان را یاد می‌گیرند، اما برای رسیدن به عملکرد واقعی در یک کاربرد مشخص، به **پس‌آموزش** نیاز دارند. بیشتر کارهای پس‌آموزش بر ارزیابی‌های ذهنی و ترجیحات انسانی تکیه می‌کنند (مثلاً RLHF). این مقاله یک سؤال عملی‌تر را طرح می‌کند: **آیا می‌توان با استفاده از بازخورد عملکرد واقعی (مثل CTR) در یک محصول واقعی، اثر اقتصادی قابل‌اندازه‌گیری ایجاد کرد؟**

حوزه تبلیغات آنلاین از نظر اقتصادی بسیار حساس است و حتی تغییرات کوچک در CTR می‌تواند بازگشت سرمایه قابل توجهی ایجاد کند. بنابراین، این مقاله تلاش می‌کند نشان دهد **RL مبتنی بر متریک عملکرد** می‌تواند در عمل اثرگذار باشد، نه فقط در آزمایشگاه.

## ۲) ورودی‌ها و خروجی‌های مدل/سیستم
- **ورودی:** متن تبلیغ اولیه که توسط تبلیغ‌دهنده نوشته شده است.
- **خروجی:** چندین نسخه بازنویسی‌شده از متن تبلیغ که کاربر می‌تواند انتخاب یا ویرایش کند.

هدف سیستم تولید یک متن نهایی واحد نیست، بلکه ارائه مجموعه‌ای از گزینه‌هاست تا تبلیغ‌دهنده بتواند بهترین نسخه را انتخاب کند.

## ۳) داده مورد استفاده (نوع، منبع، اندازه)
روش مقاله بر داده‌های تاریخی عملکرد تبلیغات متکی است؛ داده‌هایی که در آن **فقط متن تبلیغ تغییر می‌کند** و سایر اجزای تبلیغ ثابت می‌مانند (مثل تصویر، هدف‌گیری و بودجه). این وضعیت امکان نسبت دادن تغییر عملکرد به متن تبلیغ را فراهم می‌کند.

مشخصات کلیدی:
- **نوع داده:** جفت‌های ترجیحی متن (متن بهتر با CTR بالاتر در برابر متن بدتر).
- **منبع:** داده‌های تاریخی پلتفرم تبلیغات Meta/Facebook.
- **اندازه:** در آزمایش A/B واقعی، حدود **۳۵٬۰۰۰ تبلیغ‌دهنده** و **۶۴۰٬۰۰۰ واریانت تبلیغ** طی **۱۰ هفته**.

## ۴) روش پیشنهادی به زبان ساده (RLPF)
روش پیشنهادی «یادگیری تقویتی با بازخورد عملکرد» (RLPF) است:

1. **ساخت مدل پاداش:**
   - از داده‌های تاریخی، جفت متن‌های تبلیغ می‌سازیم (متنی که CTR بالاتری داشته برتر است).
   - یک مدل پاداش آموزش می‌دهیم که بتواند از روی متن، امتیاز عملکرد (CTR) را پیش‌بینی یا رتبه‌بندی کند.

2. **پس‌آموزش LLM با PPO:**
   - مدل زبانی متنی تولید می‌کند.
   - مدل پاداش به هر متن امتیاز می‌دهد.
   - الگوریتم PPO متن‌هایی با امتیاز بالاتر را تقویت می‌کند.
   - برای جلوگیری از متن‌های خیلی طولانی، جریمه طول اضافه می‌شود.

این روش از نظر مفهومی بین RLHF (بازخورد انسان) و RL با پاداش قابل‌اعتبارسنجی قرار می‌گیرد؛ تفاوت اینجاست که پاداش از یک **متریک واقعی کسب‌وکار** می‌آید.

### شبه‌کد ساده
```
# ۱) ساخت داده ترجیحی از داده تاریخی
pairs = [(text_a, text_b, better) for (text_a, text_b) in historical_ads]

# ۲) آموزش مدل پاداش
reward_model = train_reward_model(pairs)

# ۳) RL با PPO
for batch in prompts:
    samples = LLM.generate(prompt)
    rewards = reward_model.score(samples)
    LLM = PPO_update(LLM, samples, rewards, KL_penalty, length_penalty)
```

## ۵) مدل‌ها و مقایسه‌ها
- **Imitation LLM v1:** آموزش نظارتی روی داده‌های بازنویسی مصنوعی.
- **Imitation LLM v2:** آموزش نظارتی روی داده‌های مصنوعی + داده انسانی.
- **AdLlama (پیشنهاد مقاله):** نسخه v2 + یادگیری تقویتی با پاداش عملکردی (RLPF).

همه مدل‌ها بر پایه **Llama 2 Chat 7B** هستند.

## ۶) طراحی آزمایش (A/B Test)
- **بازه زمانی:** ۱۶ فوریه ۲۰۲۴ تا ۲۵ آوریل ۲۰۲۴ (۱۰ هفته).
- **نمونه:** ۳۴٬۸۴۹ تبلیغ‌دهنده در آمریکا.
- **تصادفی‌سازی:** در سطح تبلیغ‌دهنده.
- **معیارها:** CTR تبلیغ‌دهنده، تعداد کلیک، تعداد نمایش، تعداد تبلیغ، تعداد واریانت.

تمرکز اصلی بر **عملکرد در سطح تبلیغ‌دهنده** است تا اثر واقعی روی ROI مشخص شود.

## ۷) نتایج اصلی
- افزایش **CTR نسبی ۶.۷٪** برای AdLlama نسبت به Imitation v2 (p = 0.0296).
- افزایش مطلق CTR حدود **۳.۱٪ به ۳.۳٪**.
- افزایش **۱۸.۵٪ در تعداد واریانت‌ها** (از ~۱۶.۸ به ~۱۹.۹).
- تعداد کل تبلیغ‌ها تغییر معناداری ندارد.

در پلتفرمی مثل Facebook، حتی بهبود کوچک در CTR بسیار مهم است؛ بنابراین ۶.۷٪ افزایش نسبی قابل توجه است.

## ۸) محدودیت‌ها
- **یادگیری آفلاین:** فقط از داده تاریخی استفاده می‌شود و مدل به‌صورت آنلاین به‌روز نمی‌شود.
- **هدف تک‌معیاره:** فقط CTR بهینه شده و معیارهایی مثل خلاقیت یا لحن در نظر گرفته نشده‌اند.
- **انتخاب انسانی نادیده گرفته شده:** مدل پاداش در نظر نمی‌گیرد که تبلیغ‌دهنده کدام متن را انتخاب می‌کند.
- **اثر کلان پلتفرم:** اثر روی تنوع تبلیغات یا تجربه کاربری کلان تحلیل نشده است.

## ۹) ایده‌های ادامه
- **RL آنلاین:** افزودن حلقه بازخورد واقعی و پیوسته.
- **پاداش چندهدفه:** ترکیب CTR با خلاقیت یا لحن برند.
- **وزن‌دهی بر اساس انتخاب انسان:** ترکیب احتمال انتخاب تبلیغ‌دهنده با CTR.
- **تعمیم به حوزه‌های دیگر:** ایمیل مارکتینگ، توضیح محصول، پشتیبانی مشتری.

## ۱۰) نتیجه‌گیری
این مقاله یکی از معدود مطالعات واقعی و بزرگ‌مقیاس است که نشان می‌دهد **پس‌آموزش RL با متریک عملکردی** می‌تواند اثر اقتصادی ملموس داشته باشد. اهمیت آن در این است که نشان می‌دهد مدل‌های زبانی می‌توانند از حالت «توانمند» به حالت «اثرگذار» منتقل شوند، به‌شرط آن‌که پاداش با اهداف واقعی کسب‌وکار هماهنگ باشد.
